{'data_path': 'data/MS_COCO_right', 'kenlm_path': './kenlm', 'save': 'COCO0.08added2021-07-29-05-25-13', 'maxlen': 15, 'vocab_size': 0, 'lowercase': True, 'emsize': 512, 'nhidden': 512, 'nlayers': 2, 'noise_r': 0.05, 'nheads': 4, 'nff': 1024, 'aehidden': 56, 'noise_anneal': 0.9995, 'hidden_init': False, 'arch_g': '300-300', 'arch_d': '300-300', 'arch_d_local': '300-300', 'z_size': 100, 'dropout': 0.3, 'noise_seq_length': 15, 'gan_type': 'kl', 'epochs': 100, 'min_epochs': 12, 'no_earlystopping': False, 'patience': 2, 'batch_size': 256, 'eval_batch_size': 32, 'niters_ae': 1, 'niters_gan_d': 1, 'niters_gan_dec': 1, 'niters_gan_g': 1, 'niters_gan_ae': 1, 'niters_gan_schedule': '', 'lr_ae': 0.08, 'lr_gan_e': 0.0001, 'lr_gan_g': 0.0004, 'lr_gan_d': 0.0001, 'beta1': 0.5, 'clip': 1, 'gan_clamp': 0.01, 'gan_gp_lambda': 1, 'gan_lambda': 0.1, 'add_noise': True, 'gan_d_local': True, 'gan_d_local_windowsize': 3, 'gan_g_activation': False, 'enhance_dec': False, 'sample': False, 'N': 5, 'log_interval': 200, 'seed': 1111, 'ntokens': 13548}
Training
| epoch   1 |     0/  453 batches | lr 0.000000 | ms/batch  1.47 | loss  0.05 | ppl     1.05 | acc     0.00 | train_ae_norm     1.00
[1/100][199/453] Loss_D: 1.37535095 (Loss_D_real: 0.68222713 Loss_D_fake: 0.69312382) Loss_G: 0.00070856 Loss_Enh_Dec: 0.00000000
| epoch   1 |   200/  453 batches | lr 0.000000 | ms/batch 255.38 | loss  5.93 | ppl   377.11 | acc     0.28 | train_ae_norm     1.00
[1/100][399/453] Loss_D: 1.38030505 (Loss_D_real: 0.68916070 Loss_D_fake: 0.69114429) Loss_G: -0.00023106 Loss_Enh_Dec: 0.00000000
| epoch   1 |   400/  453 batches | lr 0.000000 | ms/batch 259.08 | loss  4.81 | ppl   122.80 | acc     0.32 | train_ae_norm     1.00
| end of epoch   1 | time: 122.58s | test loss 681.50 | test ppl 93855154031515658530128656698768368557882571690593677140345317144692161507417768955820257190402656378648735509897419636682796758524971112005996835265432607951536474720996116250184036689593022975559979422450299158424347591825610278955470755682952163039297697766825413873387987051531977688126324736.00 | acc 0.337
| epoch   2 |     0/  453 batches | lr 0.000000 | ms/batch  0.92 | loss  0.02 | ppl     1.02 | acc     0.33 | train_ae_norm     1.00
[2/100][199/453] Loss_D: 1.38398314 (Loss_D_real: 0.69072926 Loss_D_fake: 0.69325387) Loss_G: 0.00002404 Loss_Enh_Dec: 0.00000000
| epoch   2 |   200/  453 batches | lr 0.000000 | ms/batch 262.15 | loss  4.38 | ppl    79.94 | acc     0.35 | train_ae_norm     1.00
[2/100][399/453] Loss_D: 1.38366711 (Loss_D_real: 0.69246423 Loss_D_fake: 0.69120288) Loss_G: 0.00018854 Loss_Enh_Dec: 0.00000000
| epoch   2 |   400/  453 batches | lr 0.000000 | ms/batch 246.64 | loss  4.21 | ppl    67.39 | acc     0.35 | train_ae_norm     1.00
| end of epoch   2 | time: 120.96s | test loss 622.38 | test ppl 1982844075481146015321065645155686916058623435802486547829807004278089036425964913058157259781605696588913907339604813968794563212319308914411351714441817959340411322560010796985471064146548690829194953065528840655744895059080764713803473406580773831929629594169064292352.00 | acc 0.361
| epoch   3 |     0/  453 batches | lr 0.000000 | ms/batch  0.85 | loss  0.02 | ppl     1.02 | acc     0.35 | train_ae_norm     1.00
[3/100][199/453] Loss_D: 1.38102615 (Loss_D_real: 0.68965602 Loss_D_fake: 0.69137013) Loss_G: 0.00028994 Loss_Enh_Dec: 0.00000000
| epoch   3 |   200/  453 batches | lr 0.000000 | ms/batch 244.49 | loss  4.03 | ppl    56.31 | acc     0.36 | train_ae_norm     1.00
[3/100][399/453] Loss_D: 1.38305569 (Loss_D_real: 0.69157064 Loss_D_fake: 0.69148511) Loss_G: 0.00032838 Loss_Enh_Dec: 0.00000000
| epoch   3 |   400/  453 batches | lr 0.000000 | ms/batch 248.32 | loss  3.95 | ppl    51.69 | acc     0.36 | train_ae_norm     1.00
| end of epoch   3 | time: 118.32s | test loss 590.57 | test ppl 30171638503761337658095982209864273089074477847797359590418503238697881828352839076981366122399828158640706520938185578503467362841731644983554257794439785573739706478273508565395865482868990198088131288414586867846269075337156618904299755532360220160294912.00 | acc 0.378
| epoch   4 |     0/  453 batches | lr 0.000000 | ms/batch  0.83 | loss  0.02 | ppl     1.02 | acc     0.36 | train_ae_norm     1.00
[4/100][199/453] Loss_D: 1.37819552 (Loss_D_real: 0.68942189 Loss_D_fake: 0.68877357) Loss_G: 0.00066818 Loss_Enh_Dec: 0.00000000
| epoch   4 |   200/  453 batches | lr 0.000000 | ms/batch 241.33 | loss  3.82 | ppl    45.65 | acc     0.37 | train_ae_norm     1.00
[4/100][399/453] Loss_D: 1.37751770 (Loss_D_real: 0.68810666 Loss_D_fake: 0.68941104) Loss_G: 0.00070682 Loss_Enh_Dec: 0.00000000
| epoch   4 |   400/  453 batches | lr 0.000000 | ms/batch 254.76 | loss  3.77 | ppl    43.28 | acc     0.37 | train_ae_norm     1.00
| end of epoch   4 | time: 119.34s | test loss 568.21 | test ppl 5904631514049700231377141031812430250218403477366943533601887890584285695710883561977207848989068404628413502995983993419539217917261580811823514689583084086135670394067656957375144643728332851154636733068982583734517077056615470338746701773275136.00 | acc 0.388
bleu_self: [0.99368687,0.99128221,0.98825653,0.98406572,0.98017242]
bleu_test: [1.00000000,0.99037812,0.99352926,0.98773736,0.95365776]
| epoch   5 |     0/  453 batches | lr 0.000000 | ms/batch  1.06 | loss  0.02 | ppl     1.02 | acc     0.38 | train_ae_norm     1.00
[5/100][199/453] Loss_D: 1.36929655 (Loss_D_real: 0.68372428 Loss_D_fake: 0.68557227) Loss_G: 0.00122166 Loss_Enh_Dec: 0.00000000
| epoch   5 |   200/  453 batches | lr 0.000000 | ms/batch 257.53 | loss  3.68 | ppl    39.60 | acc     0.38 | train_ae_norm     1.00
[5/100][399/453] Loss_D: 1.36401558 (Loss_D_real: 0.67869210 Loss_D_fake: 0.68532342) Loss_G: 0.00126300 Loss_Enh_Dec: 0.00000000
| epoch   5 |   400/  453 batches | lr 0.000000 | ms/batch 254.86 | loss  3.64 | ppl    38.26 | acc     0.38 | train_ae_norm     1.00
| end of epoch   5 | time: 123.06s | test loss 550.58 | test ppl 129429796797560886376714382921026362492592351235905341192936174856150223453330766099550505199383385207214089041991514354027928104595078148005458057927401668610974492806389975582532176938319902076961749170883155496311329217854622115198664704.00 | acc 0.395
bleu_self: [0.98493869,0.97938421,0.96863639,0.94111046,0.92977009]
bleu_test: [1.00000000,1.00000000,0.98618264,0.94207082,0.76518577]
| epoch   6 |     0/  453 batches | lr 0.000000 | ms/batch  0.71 | loss  0.02 | ppl     1.02 | acc     0.39 | train_ae_norm     1.00
[6/100][199/453] Loss_D: 1.31321311 (Loss_D_real: 0.66035211 Loss_D_fake: 0.65286094) Loss_G: 0.00904858 Loss_Enh_Dec: 0.00000000
| epoch   6 |   200/  453 batches | lr 0.000000 | ms/batch 254.16 | loss  3.57 | ppl    35.63 | acc     0.39 | train_ae_norm     1.00
[6/100][399/453] Loss_D: 1.29415631 (Loss_D_real: 0.64679492 Loss_D_fake: 0.64736134) Loss_G: 0.00840656 Loss_Enh_Dec: 0.00000000
| epoch   6 |   400/  453 batches | lr 0.000000 | ms/batch 251.84 | loss  3.56 | ppl    35.04 | acc     0.40 | train_ae_norm     1.00
| end of epoch   6 | time: 121.80s | test loss 538.47 | test ppl 714027967440147556101177391275822660775602605756271755705343169715014147549025156606033071998194525626381997831014469372787647680734107797761020974968900859731954338235702678037844787390153616081878500864717208280310819119529189179392.00 | acc 0.401
| epoch   7 |     0/  453 batches | lr 0.000000 | ms/batch  0.81 | loss  0.02 | ppl     1.02 | acc     0.39 | train_ae_norm     1.00
[7/100][199/453] Loss_D: 1.26062512 (Loss_D_real: 0.63909829 Loss_D_fake: 0.62152690) Loss_G: 0.00556016 Loss_Enh_Dec: 0.00000000
| epoch   7 |   200/  453 batches | lr 0.000000 | ms/batch 259.68 | loss  3.50 | ppl    32.96 | acc     0.40 | train_ae_norm     1.00
[7/100][399/453] Loss_D: 1.25223875 (Loss_D_real: 0.61295044 Loss_D_fake: 0.63928837) Loss_G: -0.01204862 Loss_Enh_Dec: 0.00000000
| epoch   7 |   400/  453 batches | lr 0.000000 | ms/batch 254.13 | loss  3.49 | ppl    32.69 | acc     0.40 | train_ae_norm     1.00
| end of epoch   7 | time: 124.22s | test loss 529.96 | test ppl 143501904813942568182978300735099537083922400317707798186010655338340656418720479959531421123647595423261726806776038738073507106326930662988646421663812184591248425124534123463816333314960393051160396116917039558252897769250357248.00 | acc 0.404
| epoch   8 |     0/  453 batches | lr 0.000000 | ms/batch  1.06 | loss  0.02 | ppl     1.02 | acc     0.40 | train_ae_norm     1.00
[8/100][199/453] Loss_D: 1.13565588 (Loss_D_real: 0.55695117 Loss_D_fake: 0.57870471) Loss_G: -0.02692854 Loss_Enh_Dec: 0.00000000
| epoch   8 |   200/  453 batches | lr 0.000000 | ms/batch 261.78 | loss  3.43 | ppl    30.80 | acc     0.39 | train_ae_norm     1.00
[8/100][399/453] Loss_D: 1.08092761 (Loss_D_real: 0.55885875 Loss_D_fake: 0.52206886) Loss_G: -0.01567275 Loss_Enh_Dec: 0.00000000
| epoch   8 |   400/  453 batches | lr 0.000000 | ms/batch 253.00 | loss  3.42 | ppl    30.46 | acc     0.40 | train_ae_norm     1.00
| end of epoch   8 | time: 124.64s | test loss 521.59 | test ppl 33259904801075606038844745854929717144664255527711223914547274020879127287222953669319107220229331359879731258460355182164094834710877421054110388736050434036123943468163232573475843605242382380910159302596993232408075827150848.00 | acc 0.407
bleu_self: [0.98238636,0.97794915,0.97240663,0.96652843,0.96079509]
bleu_test: [1.00000000,0.99839635,0.99649700,0.98108671,0.96103535]
| epoch   9 |     0/  453 batches | lr 0.000000 | ms/batch  0.95 | loss  0.02 | ppl     1.02 | acc     0.41 | train_ae_norm     1.00
[9/100][199/453] Loss_D: 1.06720114 (Loss_D_real: 0.52711213 Loss_D_fake: 0.54008907) Loss_G: 0.00209402 Loss_Enh_Dec: 0.00000000
| epoch   9 |   200/  453 batches | lr 0.000000 | ms/batch 265.44 | loss  3.37 | ppl    29.12 | acc     0.40 | train_ae_norm     1.00
[9/100][399/453] Loss_D: 0.98341656 (Loss_D_real: 0.47667947 Loss_D_fake: 0.50673711) Loss_G: 0.00543423 Loss_Enh_Dec: 0.00000000
| epoch   9 |   400/  453 batches | lr 0.000000 | ms/batch 256.09 | loss  3.37 | ppl    28.98 | acc     0.41 | train_ae_norm     1.00
| end of epoch   9 | time: 125.29s | test loss 515.49 | test ppl 75152362922075373737905248021280673187723685971207897956556929972342740532884546040378879991232273900101778786797806090114051974336430772733083508054985778464571003759001139849513761410263140486694365370232877822233121128448.00 | acc 0.411
| epoch  10 |     0/  453 batches | lr 0.000000 | ms/batch  1.12 | loss  0.02 | ppl     1.02 | acc     0.41 | train_ae_norm     1.00
[10/100][199/453] Loss_D: 0.85705173 (Loss_D_real: 0.44857535 Loss_D_fake: 0.40847641) Loss_G: 0.01566909 Loss_Enh_Dec: 0.00000000
| epoch  10 |   200/  453 batches | lr 0.000000 | ms/batch 254.46 | loss  3.33 | ppl    27.81 | acc     0.40 | train_ae_norm     1.00
[10/100][399/453] Loss_D: 0.98027778 (Loss_D_real: 0.49720293 Loss_D_fake: 0.48307481) Loss_G: -0.00424693 Loss_Enh_Dec: 0.00000000
| epoch  10 |   400/  453 batches | lr 0.000000 | ms/batch 268.44 | loss  3.33 | ppl    27.83 | acc     0.42 | train_ae_norm     1.00
| end of epoch  10 | time: 125.15s | test loss 510.65 | test ppl 591179106888972669748618884399600489471664862718720411435856411377139942926547055518243530863421544761782027193259441733905479136961226320673096266595744177472529037205901020871130083931921893584872565775710073693092708352.00 | acc 0.413
bleu_self: [0.99715909,0.99540009,0.99459447,0.99397049,0.99336202]
bleu_test: [1.00000000,1.00000000,1.00000000,1.00000000,0.98730691]
| epoch  11 |     0/  453 batches | lr 0.000000 | ms/batch  0.67 | loss  0.02 | ppl     1.02 | acc     0.41 | train_ae_norm     1.00
[11/100][199/453] Loss_D: 1.14700294 (Loss_D_real: 0.55216426 Loss_D_fake: 0.59483874) Loss_G: -0.03211443 Loss_Enh_Dec: 0.00000000
| epoch  11 |   200/  453 batches | lr 0.000000 | ms/batch 255.08 | loss  3.29 | ppl    26.78 | acc     0.40 | train_ae_norm     1.00
[11/100][399/453] Loss_D: 0.89852965 (Loss_D_real: 0.43473107 Loss_D_fake: 0.46379855) Loss_G: -0.00928250 Loss_Enh_Dec: 0.00000000
| epoch  11 |   400/  453 batches | lr 0.000000 | ms/batch 253.52 | loss  3.29 | ppl    26.81 | acc     0.42 | train_ae_norm     1.00
| end of epoch  11 | time: 121.50s | test loss 506.31 | test ppl 7733102513113532532322983980604823259217175896314788823575804288326714624446960892034733713004626716163713475293883553987955908553657832997393322731506531517444172501924657852881774141490753788325654895232242459288272896.00 | acc 0.414
| epoch  12 |     0/  453 batches | lr 0.000000 | ms/batch  0.88 | loss  0.02 | ppl     1.02 | acc     0.41 | train_ae_norm     1.00
[12/100][199/453] Loss_D: 0.81835437 (Loss_D_real: 0.41892239 Loss_D_fake: 0.39943194) Loss_G: 0.02922939 Loss_Enh_Dec: 0.00000000
| epoch  12 |   200/  453 batches | lr 0.000000 | ms/batch 262.75 | loss  3.25 | ppl    25.84 | acc     0.41 | train_ae_norm     1.00
[12/100][399/453] Loss_D: 0.71113569 (Loss_D_real: 0.35306734 Loss_D_fake: 0.35806835) Loss_G: 0.06411366 Loss_Enh_Dec: 0.00000000
| epoch  12 |   400/  453 batches | lr 0.000000 | ms/batch 266.61 | loss  3.26 | ppl    25.93 | acc     0.41 | train_ae_norm     1.00
| end of epoch  12 | time: 127.27s | test loss 503.42 | test ppl 430794485137316460072473397353595711305657515953257837539724485777450545476253821964552470464126375138917310132527368763074791922299170835224059379145093764143224609184139715864005284595112669938900245828952164164173824.00 | acc 0.416
bleu_self: [0.98921827,0.98893199,0.98859541,0.98819065,0.98768863]
bleu_test: [1.00000000,1.00000000,1.00000000,0.99881861,0.99796625]
| epoch  13 |     0/  453 batches | lr 0.000000 | ms/batch  1.18 | loss  0.02 | ppl     1.02 | acc     0.41 | train_ae_norm     1.00
[13/100][199/453] Loss_D: 0.80511129 (Loss_D_real: 0.31518906 Loss_D_fake: 0.48992220) Loss_G: 0.07866000 Loss_Enh_Dec: 0.00000000
| epoch  13 |   200/  453 batches | lr 0.000000 | ms/batch 248.69 | loss  3.22 | ppl    25.01 | acc     0.41 | train_ae_norm     1.00
[13/100][399/453] Loss_D: 0.52971178 (Loss_D_real: 0.24150164 Loss_D_fake: 0.28821015) Loss_G: 0.07182308 Loss_Enh_Dec: 0.00000000
| epoch  13 |   400/  453 batches | lr 0.000000 | ms/batch 253.34 | loss  3.22 | ppl    25.12 | acc     0.42 | train_ae_norm     1.00
| end of epoch  13 | time: 120.97s | test loss 499.79 | test ppl 11402769236812607231818700312911968064270281236933026644209933964207884224201070243683005183652068991668020793000586625649106329983515296121587358485269434227017127645830815527509193254010631441757572479226944679837696.00 | acc 0.417
| epoch  14 |     0/  453 batches | lr 0.000000 | ms/batch  0.71 | loss  0.02 | ppl     1.02 | acc     0.43 | train_ae_norm     1.00
[14/100][199/453] Loss_D: 0.43255904 (Loss_D_real: 0.17575982 Loss_D_fake: 0.25679922) Loss_G: 0.05910287 Loss_Enh_Dec: 0.00000000
| epoch  14 |   200/  453 batches | lr 0.000000 | ms/batch 253.99 | loss  3.19 | ppl    24.38 | acc     0.42 | train_ae_norm     1.00
[14/100][399/453] Loss_D: 0.39699900 (Loss_D_real: 0.19713128 Loss_D_fake: 0.19986771) Loss_G: 0.07999009 Loss_Enh_Dec: 0.00000000
| epoch  14 |   400/  453 batches | lr 0.000000 | ms/batch 257.51 | loss  3.20 | ppl    24.44 | acc     0.42 | train_ae_norm     1.00
| end of epoch  14 | time: 122.17s | test loss 496.28 | test ppl 341216537168667845456322761103053494440452926526984447197815342152616024588451536309831636854548412319845563122125663461369558161928761985424472910388909265372117533355233903932635882779958525744258580047007838109696.00 | acc 0.420
| epoch  15 |     0/  453 batches | lr 0.000000 | ms/batch  1.16 | loss  0.02 | ppl     1.02 | acc     0.42 | train_ae_norm     1.00
[15/100][199/453] Loss_D: 0.32034680 (Loss_D_real: 0.14269790 Loss_D_fake: 0.17764890) Loss_G: 0.10247260 Loss_Enh_Dec: 0.00000000
| epoch  15 |   200/  453 batches | lr 0.000000 | ms/batch 250.50 | loss  3.17 | ppl    23.71 | acc     0.42 | train_ae_norm     1.00
[15/100][399/453] Loss_D: 0.30021909 (Loss_D_real: 0.13786927 Loss_D_fake: 0.16234982) Loss_G: 0.09069972 Loss_Enh_Dec: 0.00000000
| epoch  15 |   400/  453 batches | lr 0.000000 | ms/batch 252.15 | loss  3.17 | ppl    23.82 | acc     0.42 | train_ae_norm     1.00
| end of epoch  15 | time: 120.94s | test loss 494.76 | test ppl 74344788118935290526945845482948551721748801558001758266988303240870441377030760191968044384539584864169046023160389541293095995265593527394454426062670602715173545660497879831783845923293934283713568729192722333696.00 | acc 0.420
bleu_self: [1.00000000,0.99821278,0.99604939,0.99470943,0.99359208]
bleu_test: [1.00000000,1.00000000,1.00000000,0.99763722,0.94724994]
New saving model: epoch 015.
| epoch  16 |     0/  453 batches | lr 0.000000 | ms/batch  0.77 | loss  0.02 | ppl     1.02 | acc     0.42 | train_ae_norm     1.00
[16/100][199/453] Loss_D: 0.22684622 (Loss_D_real: 0.11496475 Loss_D_fake: 0.11188146) Loss_G: 0.09651830 Loss_Enh_Dec: 0.00000000
| epoch  16 |   200/  453 batches | lr 0.000000 | ms/batch 245.24 | loss  3.14 | ppl    23.17 | acc     0.42 | train_ae_norm     1.00
[16/100][399/453] Loss_D: 0.24172302 (Loss_D_real: 0.12171920 Loss_D_fake: 0.12000382) Loss_G: 0.08720515 Loss_Enh_Dec: 0.00000000
| epoch  16 |   400/  453 batches | lr 0.000000 | ms/batch 245.47 | loss  3.15 | ppl    23.25 | acc     0.42 | train_ae_norm     1.00
| end of epoch  16 | time: 119.12s | test loss 492.08 | test ppl 5084765800792005552688611319893360369304740500697685490475781268797985239080622482383070824424796531554817871776155646430534970011211992367677622963181536618122038746128584601549644174560515216353948467164767846400.00 | acc 0.421
bleu_self: [0.98758013,0.98703992,0.98642799,0.98572581,0.98490688]
bleu_test: [1.00000000,1.00000000,1.00000000,1.00000000,0.95679559]
| epoch  17 |     0/  453 batches | lr 0.000000 | ms/batch  0.72 | loss  0.02 | ppl     1.02 | acc     0.42 | train_ae_norm     1.00
[17/100][199/453] Loss_D: 0.21726665 (Loss_D_real: 0.08399442 Loss_D_fake: 0.13327223) Loss_G: 0.12874405 Loss_Enh_Dec: 0.00000000
| epoch  17 |   200/  453 batches | lr 0.000000 | ms/batch 237.14 | loss  3.12 | ppl    22.63 | acc     0.42 | train_ae_norm     1.00
[17/100][399/453] Loss_D: 0.15927568 (Loss_D_real: 0.07387779 Loss_D_fake: 0.08539789) Loss_G: 0.09743301 Loss_Enh_Dec: 0.00000000
| epoch  17 |   400/  453 batches | lr 0.000000 | ms/batch 248.15 | loss  3.12 | ppl    22.74 | acc     0.43 | train_ae_norm     1.00
| end of epoch  17 | time: 117.14s | test loss 489.92 | test ppl 585344200616737223915002649231533587101063577914527126372449401881738654549794159012137474119967618001674576686427177536055452408804682448612444873607028047616042962901457037289104437260469631487177643498074537984.00 | acc 0.421
| epoch  18 |     0/  453 batches | lr 0.000000 | ms/batch  0.80 | loss  0.02 | ppl     1.02 | acc     0.42 | train_ae_norm     1.00
[18/100][199/453] Loss_D: 0.15330555 (Loss_D_real: 0.08328480 Loss_D_fake: 0.07002075) Loss_G: 0.13061500 Loss_Enh_Dec: 0.00000000
| epoch  18 |   200/  453 batches | lr 0.000000 | ms/batch 242.33 | loss  3.10 | ppl    22.16 | acc     0.43 | train_ae_norm     1.00
[18/100][399/453] Loss_D: 0.11369176 (Loss_D_real: 0.05829954 Loss_D_fake: 0.05539222) Loss_G: 0.12569898 Loss_Enh_Dec: 0.00000000
| epoch  18 |   400/  453 batches | lr 0.000000 | ms/batch 257.40 | loss  3.11 | ppl    22.31 | acc     0.43 | train_ae_norm     1.00
| end of epoch  18 | time: 119.26s | test loss 488.71 | test ppl 174997567322552166814926751128010815282681757956549422165034981894718120823512482869965777048424441830653644548512564478376215997707329485003617556462372404313777361076211368876878896592227443952723280769086128128.00 | acc 0.422
| epoch  19 |     0/  453 batches | lr 0.000000 | ms/batch  0.76 | loss  0.02 | ppl     1.02 | acc     0.42 | train_ae_norm     1.00
[19/100][199/453] Loss_D: 0.11107946 (Loss_D_real: 0.04786133 Loss_D_fake: 0.06321813) Loss_G: 0.14554112 Loss_Enh_Dec: 0.00000000
| epoch  19 |   200/  453 batches | lr 0.000000 | ms/batch 243.66 | loss  3.08 | ppl    21.73 | acc     0.42 | train_ae_norm     1.00
[19/100][399/453] Loss_D: 0.11779776 (Loss_D_real: 0.05638274 Loss_D_fake: 0.06141502) Loss_G: 0.13333733 Loss_Enh_Dec: 0.00000000
| epoch  19 |   400/  453 batches | lr 0.000000 | ms/batch 249.30 | loss  3.09 | ppl    21.87 | acc     0.43 | train_ae_norm     1.00
| end of epoch  19 | time: 118.95s | test loss 486.60 | test ppl 21366104494854737009596330892299492006338451919233138030647635077830718082669571181768721238082828524636174778291751582120466591502239345158901347119596797734044334777966068335036753050983753374087343884489195520.00 | acc 0.423
| epoch  20 |     0/  453 batches | lr 0.000000 | ms/batch  0.88 | loss  0.02 | ppl     1.02 | acc     0.43 | train_ae_norm     1.00
[20/100][199/453] Loss_D: 0.07709236 (Loss_D_real: 0.04359359 Loss_D_fake: 0.03349877) Loss_G: 0.16284795 Loss_Enh_Dec: 0.00000000
| epoch  20 |   200/  453 batches | lr 0.000000 | ms/batch 253.77 | loss  3.06 | ppl    21.33 | acc     0.43 | train_ae_norm     1.00
[20/100][399/453] Loss_D: 0.10808662 (Loss_D_real: 0.04784251 Loss_D_fake: 0.06024411) Loss_G: 0.16866966 Loss_Enh_Dec: 0.00000000
| epoch  20 |   400/  453 batches | lr 0.000000 | ms/batch 257.19 | loss  3.07 | ppl    21.48 | acc     0.43 | train_ae_norm     1.00
| end of epoch  20 | time: 123.44s | test loss 485.62 | test ppl 7960573752505295341863902535592541043271432956919279460740812581749181069467833374807091088738318819548495045460554720509688887338037382649884092501229848624713404191118337446551912997628534276492983650538749952.00 | acc 0.424
bleu_self: [1.00000000,1.00000000,1.00000000,1.00000000,1.00000000]
bleu_test: [1.00000000,1.00000000,1.00000000,1.00000000,0.96025583]
| epoch  21 |     0/  453 batches | lr 0.000000 | ms/batch  0.98 | loss  0.02 | ppl     1.02 | acc     0.42 | train_ae_norm     1.00
[21/100][199/453] Loss_D: 0.10396484 (Loss_D_real: 0.05108413 Loss_D_fake: 0.05288071) Loss_G: 0.16788219 Loss_Enh_Dec: 0.00000000
| epoch  21 |   200/  453 batches | lr 0.000000 | ms/batch 244.28 | loss  3.04 | ppl    20.97 | acc     0.43 | train_ae_norm     1.00
[21/100][399/453] Loss_D: 0.08764151 (Loss_D_real: 0.04679796 Loss_D_fake: 0.04084355) Loss_G: 0.22016998 Loss_Enh_Dec: 0.00000000
| epoch  21 |   400/  453 batches | lr 0.000000 | ms/batch 244.35 | loss  3.05 | ppl    21.11 | acc     0.43 | train_ae_norm     1.00
| end of epoch  21 | time: 117.51s | test loss 484.51 | test ppl 2635553882194691074820099859277664400813909329712972912431531631729075121363470111089661787167556458015605900861425905607562777749719309669802316915104892916230009695840031543885977858701477016364166231380459520.00 | acc 0.425
| epoch  22 |     0/  453 batches | lr 0.000000 | ms/batch  0.84 | loss  0.02 | ppl     1.02 | acc     0.43 | train_ae_norm     1.00
[22/100][199/453] Loss_D: 0.37943560 (Loss_D_real: 0.21788292 Loss_D_fake: 0.16155270) Loss_G: 0.11437970 Loss_Enh_Dec: 0.00000000
| epoch  22 |   200/  453 batches | lr 0.000000 | ms/batch 243.39 | loss  3.03 | ppl    20.60 | acc     0.43 | train_ae_norm     1.00
[22/100][399/453] Loss_D: 0.78807336 (Loss_D_real: 0.68761718 Loss_D_fake: 0.10045619) Loss_G: 0.13813041 Loss_Enh_Dec: 0.00000000
| epoch  22 |   400/  453 batches | lr 0.000000 | ms/batch 237.61 | loss  3.03 | ppl    20.77 | acc     0.43 | train_ae_norm     1.00
| end of epoch  22 | time: 115.33s | test loss 482.84 | test ppl 492763721883317474094928442111057661535870114371388798831836956135930239140687572328946147543175101644652985678189075309209499753881883255467565019286348445699290409424575842180819893210564305942150696124022784.00 | acc 0.425
| epoch  23 |     0/  453 batches | lr 0.000000 | ms/batch  0.98 | loss  0.02 | ppl     1.02 | acc     0.44 | train_ae_norm     1.00
[23/100][199/453] Loss_D: 0.03539567 (Loss_D_real: 0.01447151 Loss_D_fake: 0.02092416) Loss_G: 0.22558013 Loss_Enh_Dec: 0.00000000
| epoch  23 |   200/  453 batches | lr 0.000000 | ms/batch 243.48 | loss  3.01 | ppl    20.27 | acc     0.44 | train_ae_norm     1.00
[23/100][399/453] Loss_D: 0.04961255 (Loss_D_real: 0.02694320 Loss_D_fake: 0.02266935) Loss_G: 0.15375325 Loss_Enh_Dec: 0.00000000
| epoch  23 |   400/  453 batches | lr 0.000000 | ms/batch 242.28 | loss  3.02 | ppl    20.42 | acc     0.44 | train_ae_norm     1.00
| end of epoch  23 | time: 116.42s | test loss 481.38 | test ppl 115304468266549989959684691331296480581641173599859451425823175392596082110232253016070598801687191179395042107566283213406807988641138899423735996929213975299748941958342718795835116064231195291308585595699200.00 | acc 0.426
| epoch  24 |     0/  453 batches | lr 0.000000 | ms/batch  0.88 | loss  0.02 | ppl     1.02 | acc     0.43 | train_ae_norm     1.00
[24/100][199/453] Loss_D: 0.15803260 (Loss_D_real: 0.08634182 Loss_D_fake: 0.07169078) Loss_G: 0.16874576 Loss_Enh_Dec: 0.00000000
| epoch  24 |   200/  453 batches | lr 0.000000 | ms/batch 243.64 | loss  2.99 | ppl    19.98 | acc     0.43 | train_ae_norm     1.00
[24/100][399/453] Loss_D: 0.07242258 (Loss_D_real: 0.03394901 Loss_D_fake: 0.03847357) Loss_G: 0.18007214 Loss_Enh_Dec: 0.00000000
| epoch  24 |   400/  453 batches | lr 0.000000 | ms/batch 239.58 | loss  3.00 | ppl    20.13 | acc     0.43 | train_ae_norm     1.00
| end of epoch  24 | time: 114.97s | test loss 481.37 | test ppl 114103810291080906027374689462492090664081262417855341009588019572203755063064718834888801542491338985432871220528369868199923153224196381182915634868253266032791108508304977903400777360963618880816123148763136.00 | acc 0.427
bleu_self: [1.00000000,1.00000000,1.00000000,1.00000000,1.00000000]
bleu_test: [1.00000000,1.00000000,1.00000000,1.00000000,0.95635250]
| epoch  25 |     0/  453 batches | lr 0.000000 | ms/batch  0.58 | loss  0.02 | ppl     1.02 | acc     0.43 | train_ae_norm     1.00
[25/100][199/453] Loss_D: 0.11200410 (Loss_D_real: 0.05736362 Loss_D_fake: 0.05464048) Loss_G: 0.19511817 Loss_Enh_Dec: 0.00000000
| epoch  25 |   200/  453 batches | lr 0.000000 | ms/batch 240.71 | loss  2.98 | ppl    19.72 | acc     0.43 | train_ae_norm     1.00
[25/100][399/453] Loss_D: 0.21950375 (Loss_D_real: 0.13042223 Loss_D_fake: 0.08908151) Loss_G: 0.11386198 Loss_Enh_Dec: 0.00000000
| epoch  25 |   400/  453 batches | lr 0.000000 | ms/batch 241.56 | loss  2.99 | ppl    19.84 | acc     0.44 | train_ae_norm     1.00
| end of epoch  25 | time: 114.29s | test loss 479.51 | test ppl 17653935769239881217396699091439046837583390800924720767647896762316627734980009288776895378111849105893823270031507351504876456049975813236555840553408734462464069380030293109805313346165350803555018415800320.00 | acc 0.427
bleu_self: [1.00000000,1.00000000,1.00000000,1.00000000,1.00000000]
bleu_test: [1.00000000,1.00000000,1.00000000,1.00000000,0.92531834]
| epoch  26 |     0/  453 batches | lr 0.000000 | ms/batch  0.83 | loss  0.01 | ppl     1.02 | acc     0.44 | train_ae_norm     1.00
[26/100][199/453] Loss_D: 0.16186900 (Loss_D_real: 0.08715649 Loss_D_fake: 0.07471251) Loss_G: 0.10592066 Loss_Enh_Dec: 0.00000000
| epoch  26 |   200/  453 batches | lr 0.000000 | ms/batch 240.47 | loss  2.97 | ppl    19.43 | acc     0.43 | train_ae_norm     1.00
[26/100][399/453] Loss_D: 0.17314377 (Loss_D_real: 0.08680414 Loss_D_fake: 0.08633964) Loss_G: 0.12783614 Loss_Enh_Dec: 0.00000000
| epoch  26 |   400/  453 batches | lr 0.000000 | ms/batch 240.41 | loss  2.98 | ppl    19.68 | acc     0.44 | train_ae_norm     1.00
| end of epoch  26 | time: 114.13s | test loss 478.09 | test ppl 4302845806981115315807619924396997401864094456660023898442844766955319365680924398081303774125925438478214362333492035490683635300607275338288467093389303168895542078988165960598960496305718785843581308370944.00 | acc 0.427
| epoch  27 |     0/  453 batches | lr 0.000000 | ms/batch  0.92 | loss  0.01 | ppl     1.01 | acc     0.44 | train_ae_norm     1.00
[27/100][199/453] Loss_D: 0.14744042 (Loss_D_real: 0.06336322 Loss_D_fake: 0.08407719) Loss_G: 0.16303596 Loss_Enh_Dec: 0.00000000
| epoch  27 |   200/  453 batches | lr 0.000000 | ms/batch 243.57 | loss  2.96 | ppl    19.20 | acc     0.43 | train_ae_norm     1.00
[27/100][399/453] Loss_D: 0.10826369 (Loss_D_real: 0.04986320 Loss_D_fake: 0.05840049) Loss_G: 0.11555035 Loss_Enh_Dec: 0.00000000
| epoch  27 |   400/  453 batches | lr 0.000000 | ms/batch 242.30 | loss  2.96 | ppl    19.34 | acc     0.44 | train_ae_norm     1.00
| end of epoch  27 | time: 114.96s | test loss 478.22 | test ppl 4887979660635004373549338617716577476234869156744403551436460875446961238648955040000049071829605049532424866952073047855499074138286567287628105701364016105110053177579088439639684125082528960043483542847488.00 | acc 0.429
| epoch  28 |     0/  453 batches | lr 0.000000 | ms/batch  0.90 | loss  0.01 | ppl     1.02 | acc     0.44 | train_ae_norm     1.00
[28/100][199/453] Loss_D: 0.11771161 (Loss_D_real: 0.05615861 Loss_D_fake: 0.06155301) Loss_G: 0.14045581 Loss_Enh_Dec: 0.00000000
| epoch  28 |   200/  453 batches | lr 0.000000 | ms/batch 242.17 | loss  2.94 | ppl    18.92 | acc     0.44 | train_ae_norm     1.00
[28/100][399/453] Loss_D: 0.12018712 (Loss_D_real: 0.05571213 Loss_D_fake: 0.06447499) Loss_G: 0.19993925 Loss_Enh_Dec: 0.00000000
| epoch  28 |   400/  453 batches | lr 0.000000 | ms/batch 242.43 | loss  2.95 | ppl    19.11 | acc     0.43 | train_ae_norm     1.00
| end of epoch  28 | time: 115.13s | test loss 476.86 | test ppl 1256410132007979773753146150826674408836888549682228711530354292874223115343290344261890728862106443924497916488037181260241634268132821765416877352192504828000654779675819717687255370281601011953448549089280.00 | acc 0.428
bleu_self: [1.00000000,1.00000000,1.00000000,1.00000000,1.00000000]
bleu_test: [1.00000000,1.00000000,1.00000000,1.00000000,0.96590039]
| epoch  29 |     0/  453 batches | lr 0.000000 | ms/batch  0.94 | loss  0.01 | ppl     1.01 | acc     0.44 | train_ae_norm     1.00
[29/100][199/453] Loss_D: 0.10546825 (Loss_D_real: 0.06553953 Loss_D_fake: 0.03992872) Loss_G: 0.14702660 Loss_Enh_Dec: 0.00000000
| epoch  29 |   200/  453 batches | lr 0.000000 | ms/batch 242.30 | loss  2.93 | ppl    18.71 | acc     0.44 | train_ae_norm     1.00
[29/100][399/453] Loss_D: 0.14140773 (Loss_D_real: 0.06314921 Loss_D_fake: 0.07825851) Loss_G: 0.26724416 Loss_Enh_Dec: 0.00000000
| epoch  29 |   400/  453 batches | lr 0.000000 | ms/batch 239.48 | loss  2.94 | ppl    18.92 | acc     0.44 | train_ae_norm     1.00
| end of epoch  29 | time: 114.31s | test loss 477.64 | test ppl 2737643911837389908578272128126209880224103419671885942351564256728444511951763624729755148349832424584967679296778157204553212784992890020515298747157443758121620288901853690431500125388120630246394239647744.00 | acc 0.428
| epoch  30 |     0/  453 batches | lr 0.000000 | ms/batch  0.90 | loss  0.01 | ppl     1.01 | acc     0.44 | train_ae_norm     1.00
[30/100][199/453] Loss_D: 0.08960328 (Loss_D_real: 0.03425017 Loss_D_fake: 0.05535310) Loss_G: 0.14992695 Loss_Enh_Dec: 0.00000000
| epoch  30 |   200/  453 batches | lr 0.000000 | ms/batch 242.65 | loss  2.92 | ppl    18.50 | acc     0.44 | train_ae_norm     1.00
[30/100][399/453] Loss_D: 0.10219586 (Loss_D_real: 0.07131781 Loss_D_fake: 0.03087805) Loss_G: 0.18912689 Loss_Enh_Dec: 0.00000000
| epoch  30 |   400/  453 batches | lr 0.000000 | ms/batch 244.62 | loss  2.92 | ppl    18.63 | acc     0.45 | train_ae_norm     1.00
| end of epoch  30 | time: 115.49s | test loss 475.47 | test ppl 313031667421429910692841887608756378741561673403934401783389598587377207412130718410409853936400653066279459594562891632827655823931119374615441304682118422408723338977231461473547249984363957178380245270528.00 | acc 0.429
bleu_self: [1.00000000,1.00000000,1.00000000,1.00000000,1.00000000]
bleu_test: [1.00000000,1.00000000,1.00000000,1.00000000,0.92695490]
New saving model: epoch 030.
| epoch  31 |     0/  453 batches | lr 0.000000 | ms/batch  0.86 | loss  0.01 | ppl     1.01 | acc     0.44 | train_ae_norm     1.00
[31/100][199/453] Loss_D: 0.07976662 (Loss_D_real: 0.03968910 Loss_D_fake: 0.04007752) Loss_G: 0.16587923 Loss_Enh_Dec: 0.00000000
| epoch  31 |   200/  453 batches | lr 0.000000 | ms/batch 242.65 | loss  2.91 | ppl    18.28 | acc     0.44 | train_ae_norm     1.00
[31/100][399/453] Loss_D: 0.10537319 (Loss_D_real: 0.05634327 Loss_D_fake: 0.04902992) Loss_G: 0.15819478 Loss_Enh_Dec: 0.00000000
| epoch  31 |   400/  453 batches | lr 0.000000 | ms/batch 243.57 | loss  2.91 | ppl    18.42 | acc     0.44 | train_ae_norm     1.00
| end of epoch  31 | time: 115.18s | test loss 474.96 | test ppl 186657530609335519970970898151766680326426031512434921067749026077639234900974612694498762828614986223468949289062738310579124303759065295252567867348954288806323945277769124571951059440839883341272608407552.00 | acc 0.430
| epoch  32 |     0/  453 batches | lr 0.000000 | ms/batch  0.90 | loss  0.01 | ppl     1.01 | acc     0.45 | train_ae_norm     1.00
[32/100][199/453] Loss_D: 0.05303738 (Loss_D_real: 0.02860395 Loss_D_fake: 0.02443342) Loss_G: 0.11723926 Loss_Enh_Dec: 0.00000000
| epoch  32 |   200/  453 batches | lr 0.000000 | ms/batch 243.12 | loss  2.89 | ppl    18.08 | acc     0.44 | train_ae_norm     1.00
[32/100][399/453] Loss_D: 0.07322188 (Loss_D_real: 0.03850614 Loss_D_fake: 0.03471574) Loss_G: 0.27894688 Loss_Enh_Dec: 0.00000000
| epoch  32 |   400/  453 batches | lr 0.000000 | ms/batch 246.47 | loss  2.90 | ppl    18.20 | acc     0.44 | train_ae_norm     1.00
| end of epoch  32 | time: 116.47s | test loss 473.84 | test ppl 60837861565491871191962112439479112602474270665239159894242863464977097454769500556323151092811996944336410750465189961282388689516008174881862459063501447853111519102693427749141246697668618959334905217024.00 | acc 0.430
bleu_self: [1.00000000,1.00000000,1.00000000,1.00000000,1.00000000]
bleu_test: [1.00000000,1.00000000,1.00000000,1.00000000,0.96999234]
| epoch  33 |     0/  453 batches | lr 0.000000 | ms/batch  0.91 | loss  0.01 | ppl     1.01 | acc     0.43 | train_ae_norm     1.00
[33/100][199/453] Loss_D: 0.05909429 (Loss_D_real: 0.03288565 Loss_D_fake: 0.02620864) Loss_G: 0.21006291 Loss_Enh_Dec: 0.00000000
| epoch  33 |   200/  453 batches | lr 0.000000 | ms/batch 243.89 | loss  2.88 | ppl    17.86 | acc     0.45 | train_ae_norm     1.00
[33/100][399/453] Loss_D: 0.05794701 (Loss_D_real: 0.03427478 Loss_D_fake: 0.02367223) Loss_G: 0.26291430 Loss_Enh_Dec: 0.00000000
| epoch  33 |   400/  453 batches | lr 0.000000 | ms/batch 244.97 | loss  2.89 | ppl    18.03 | acc     0.44 | train_ae_norm     1.00
| end of epoch  33 | time: 116.03s | test loss 473.26 | test ppl 34191579767315620818114718927513751663184059419250434959172686616168738694430502274818911178143248028636711993037949646528310084959755159930473865047603832866596733600773416103480480733576793662300666986496.00 | acc 0.431
| epoch  34 |     0/  453 batches | lr 0.000000 | ms/batch  0.91 | loss  0.01 | ppl     1.01 | acc     0.44 | train_ae_norm     1.00
[34/100][199/453] Loss_D: 0.06103643 (Loss_D_real: 0.02399621 Loss_D_fake: 0.03704023) Loss_G: 0.25231022 Loss_Enh_Dec: 0.00000000
| epoch  34 |   200/  453 batches | lr 0.000000 | ms/batch 242.72 | loss  2.88 | ppl    17.75 | acc     0.45 | train_ae_norm     1.00
[34/100][399/453] Loss_D: 0.04507640 (Loss_D_real: 0.02857082 Loss_D_fake: 0.01650558) Loss_G: 0.25836161 Loss_Enh_Dec: 0.00000000
| epoch  34 |   400/  453 batches | lr 0.000000 | ms/batch 242.87 | loss  2.88 | ppl    17.85 | acc     0.44 | train_ae_norm     1.00
| end of epoch  34 | time: 115.32s | test loss 473.37 | test ppl 38354738893484356706131052511258055608201079504395887191512064134899669769106665495838113929198731242071593321876472635733467617536856102675085295374300831704033304703377618122936553324635295836911400124416.00 | acc 0.430
| epoch  35 |     0/  453 batches | lr 0.000000 | ms/batch  0.96 | loss  0.01 | ppl     1.01 | acc     0.44 | train_ae_norm     1.00
[35/100][199/453] Loss_D: 0.05930832 (Loss_D_real: 0.02048399 Loss_D_fake: 0.03882433) Loss_G: 0.20539990 Loss_Enh_Dec: 0.00000000
| epoch  35 |   200/  453 batches | lr 0.000000 | ms/batch 242.74 | loss  2.86 | ppl    17.54 | acc     0.44 | train_ae_norm     1.00
[35/100][399/453] Loss_D: 0.07376231 (Loss_D_real: 0.04587810 Loss_D_fake: 0.02788420) Loss_G: 0.25383288 Loss_Enh_Dec: 0.00000000
| epoch  35 |   400/  453 batches | lr 0.000000 | ms/batch 244.69 | loss  2.87 | ppl    17.66 | acc     0.44 | train_ae_norm     1.00
| end of epoch  35 | time: 115.83s | test loss 473.28 | test ppl 34925584556942158155041691340734525540725182400400275620837184221083890065757931957153161991975023250686618941295304497108959882455420398751754365436363351325623339614948014950405951098942150696852697120768.00 | acc 0.431
bleu_self: [1.00000000,1.00000000,1.00000000,1.00000000,1.00000000]
bleu_test: [1.00000000,1.00000000,1.00000000,1.00000000,0.95635250]
| epoch  36 |     0/  453 batches | lr 0.000000 | ms/batch  0.93 | loss  0.01 | ppl     1.01 | acc     0.44 | train_ae_norm     1.00
[36/100][199/453] Loss_D: 0.03006614 (Loss_D_real: 0.01389064 Loss_D_fake: 0.01617550) Loss_G: 0.17779253 Loss_Enh_Dec: 0.00000000
| epoch  36 |   200/  453 batches | lr 0.000000 | ms/batch 243.13 | loss  2.86 | ppl    17.40 | acc     0.45 | train_ae_norm     1.00
[36/100][399/453] Loss_D: 0.07863838 (Loss_D_real: 0.04991139 Loss_D_fake: 0.02872699) Loss_G: 0.25666276 Loss_Enh_Dec: 0.00000000
| epoch  36 |   400/  453 batches | lr 0.000000 | ms/batch 241.63 | loss  2.86 | ppl    17.52 | acc     0.45 | train_ae_norm     1.00
| end of epoch  36 | time: 114.95s | test loss 472.55 | test ppl 16819472110973511092596026943326950927543405008673424262708172131478560484368831986719832485684617656488245609261962232369248732636621303942898843200792940548976987987544369181064469312150336834809016352768.00 | acc 0.431
bleu_self: [1.00000000,1.00000000,1.00000000,1.00000000,1.00000000]
bleu_test: [1.00000000,1.00000000,1.00000000,1.00000000,1.00000000]
| epoch  37 |     0/  453 batches | lr 0.000000 | ms/batch  0.91 | loss  0.01 | ppl     1.01 | acc     0.44 | train_ae_norm     1.00
[37/100][199/453] Loss_D: 0.07281198 (Loss_D_real: 0.06092143 Loss_D_fake: 0.01189055) Loss_G: 0.25973961 Loss_Enh_Dec: 0.00000000
| epoch  37 |   200/  453 batches | lr 0.000000 | ms/batch 245.40 | loss  2.84 | ppl    17.18 | acc     0.44 | train_ae_norm     1.00
[37/100][399/453] Loss_D: 0.06996357 (Loss_D_real: 0.04253721 Loss_D_fake: 0.02742637) Loss_G: 0.22038265 Loss_Enh_Dec: 0.00000000
| epoch  37 |   400/  453 batches | lr 0.000000 | ms/batch 242.44 | loss  2.85 | ppl    17.33 | acc     0.45 | train_ae_norm     1.00
| end of epoch  37 | time: 115.98s | test loss 472.08 | test ppl 10539177724414043235859037808328874325578799799037811875890978431408184176540421188622155802133424641909065965615080469297059701587361224610565726957951244082267010150687308907255056841269422438559397183488.00 | acc 0.431
| epoch  38 |     0/  453 batches | lr 0.000000 | ms/batch  0.97 | loss  0.01 | ppl     1.01 | acc     0.45 | train_ae_norm     1.00
[38/100][199/453] Loss_D: 0.07887831 (Loss_D_real: 0.03931738 Loss_D_fake: 0.03956093) Loss_G: 0.23079030 Loss_Enh_Dec: 0.00000000
| epoch  38 |   200/  453 batches | lr 0.000000 | ms/batch 245.80 | loss  2.84 | ppl    17.04 | acc     0.44 | train_ae_norm     1.00
[38/100][399/453] Loss_D: 0.30182084 (Loss_D_real: 0.03639415 Loss_D_fake: 0.26542670) Loss_G: 0.11254861 Loss_Enh_Dec: 0.00000000
| epoch  38 |   400/  453 batches | lr 0.000000 | ms/batch 239.03 | loss  2.84 | ppl    17.16 | acc     0.45 | train_ae_norm     1.00
| end of epoch  38 | time: 114.86s | test loss 471.99 | test ppl 9586956535437016217674177722468638291650226325934832945651042951548649426666408016388388455414157508169834690140672068575450486220303119061071069024397987452441348083201754857152501806036895346438021578752.00 | acc 0.431
| epoch  39 |     0/  453 batches | lr 0.000000 | ms/batch  0.92 | loss  0.01 | ppl     1.01 | acc     0.44 | train_ae_norm     1.00
[39/100][199/453] Loss_D: 0.10997213 (Loss_D_real: 0.02994359 Loss_D_fake: 0.08002855) Loss_G: 0.23993002 Loss_Enh_Dec: 0.00000000
| epoch  39 |   200/  453 batches | lr 0.000000 | ms/batch 243.51 | loss  2.83 | ppl    16.95 | acc     0.44 | train_ae_norm     1.00
[39/100][399/453] Loss_D: 0.12056704 (Loss_D_real: 0.07222441 Loss_D_fake: 0.04834263) Loss_G: 0.26498887 Loss_Enh_Dec: 0.00000000
| epoch  39 |   400/  453 batches | lr 0.000000 | ms/batch 244.31 | loss  2.84 | ppl    17.05 | acc     0.44 | train_ae_norm     1.00
| end of epoch  39 | time: 115.84s | test loss 471.51 | test ppl 5931460209418479347704657106017948972142880246393663382932547911409633307168082233464753013630882751265785170310925461011537550003353785499281087834859624623277205213296555479348091642928402685454459076608.00 | acc 0.431
| epoch  40 |     0/  453 batches | lr 0.000000 | ms/batch  0.90 | loss  0.01 | ppl     1.01 | acc     0.46 | train_ae_norm     1.00
[40/100][199/453] Loss_D: 0.06197130 (Loss_D_real: 0.03437075 Loss_D_fake: 0.02760054) Loss_G: 0.29456860 Loss_Enh_Dec: 0.00000000
| epoch  40 |   200/  453 batches | lr 0.000000 | ms/batch 246.05 | loss  2.82 | ppl    16.70 | acc     0.45 | train_ae_norm     1.00
[40/100][399/453] Loss_D: 0.09408735 (Loss_D_real: 0.07681485 Loss_D_fake: 0.01727250) Loss_G: 0.29934978 Loss_Enh_Dec: 0.00000000
| epoch  40 |   400/  453 batches | lr 0.000000 | ms/batch 239.42 | loss  2.82 | ppl    16.83 | acc     0.45 | train_ae_norm     1.00
| end of epoch  40 | time: 115.43s | test loss 471.02 | test ppl 3640797666511943148457451825422756011481945202432006112357917756153751884059202223490758409037823082807281550728823230691992810381274119041626399283503852892380875626750854726780357611207346294448814292992.00 | acc 0.431
bleu_self: [1.00000000,1.00000000,1.00000000,1.00000000,1.00000000]
bleu_test: [1.00000000,1.00000000,1.00000000,1.00000000,0.95635250]
| epoch  41 |     0/  453 batches | lr 0.000000 | ms/batch  0.95 | loss  0.01 | ppl     1.01 | acc     0.44 | train_ae_norm     1.00
[41/100][199/453] Loss_D: 0.11335056 (Loss_D_real: 0.06300943 Loss_D_fake: 0.05034113) Loss_G: 0.17157815 Loss_Enh_Dec: 0.00000000
| epoch  41 |   200/  453 batches | lr 0.000000 | ms/batch 243.05 | loss  2.81 | ppl    16.55 | acc     0.45 | train_ae_norm     1.00
[41/100][399/453] Loss_D: 0.06402870 (Loss_D_real: 0.01305687 Loss_D_fake: 0.05097183) Loss_G: 0.15724120 Loss_Enh_Dec: 0.00000000
| epoch  41 |   400/  453 batches | lr 0.000000 | ms/batch 244.38 | loss  2.82 | ppl    16.80 | acc     0.45 | train_ae_norm     1.00
| end of epoch  41 | time: 115.57s | test loss 470.89 | test ppl 3202518129867149858144325285966143681244954066268931391054373282447182490913512030846431848314745276387173225409682604098439975985060437523381974269105093330969548748829745941189154991268678385559989649408.00 | acc 0.432
| epoch  42 |     0/  453 batches | lr 0.000000 | ms/batch  0.91 | loss  0.01 | ppl     1.01 | acc     0.45 | train_ae_norm     1.00
[42/100][199/453] Loss_D: 0.10991216 (Loss_D_real: 0.07124806 Loss_D_fake: 0.03866410) Loss_G: 0.16908330 Loss_Enh_Dec: 0.00000000
| epoch  42 |   200/  453 batches | lr 0.000000 | ms/batch 244.46 | loss  2.80 | ppl    16.49 | acc     0.45 | train_ae_norm     1.00
[42/100][399/453] Loss_D: 0.03879623 (Loss_D_real: 0.02207250 Loss_D_fake: 0.01672373) Loss_G: 0.20626049 Loss_Enh_Dec: 0.00000000
| epoch  42 |   400/  453 batches | lr 0.000000 | ms/batch 243.58 | loss  2.81 | ppl    16.64 | acc     0.45 | train_ae_norm     1.00
| end of epoch  42 | time: 115.60s | test loss 470.29 | test ppl 1759715266461871412410080905678115908118048905642308748129146751555209957341913446948607752851330400710636428940008826244482042294785334078990683187484700797173840565078142797263588217735331546612998602752.00 | acc 0.431
| epoch  43 |     0/  453 batches | lr 0.000000 | ms/batch  0.91 | loss  0.01 | ppl     1.01 | acc     0.45 | train_ae_norm     1.00
[43/100][199/453] Loss_D: 0.41938764 (Loss_D_real: 0.40126541 Loss_D_fake: 0.01812222) Loss_G: 0.18159762 Loss_Enh_Dec: 0.00000000
| epoch  43 |   200/  453 batches | lr 0.000000 | ms/batch 246.16 | loss  2.79 | ppl    16.35 | acc     0.45 | train_ae_norm     1.00
[43/100][399/453] Loss_D: 0.19867039 (Loss_D_real: 0.16228203 Loss_D_fake: 0.03638836) Loss_G: 0.16510694 Loss_Enh_Dec: 0.00000000
| epoch  43 |   400/  453 batches | lr 0.000000 | ms/batch 240.82 | loss  2.80 | ppl    16.40 | acc     0.45 | train_ae_norm     1.00
| end of epoch  43 | time: 116.00s | test loss 471.06 | test ppl 3773373384643104289383766040448901781021726347803875550746527837578290117883922457721044311212015312191746756551509729896160860683427520021453024354555049727222812899767223236047280170417000680352076791808.00 | acc 0.431
| epoch  44 |     0/  453 batches | lr 0.000000 | ms/batch  0.86 | loss  0.01 | ppl     1.01 | acc     0.45 | train_ae_norm     1.00
[44/100][199/453] Loss_D: 0.24495780 (Loss_D_real: 0.19623473 Loss_D_fake: 0.04872308) Loss_G: 0.13857459 Loss_Enh_Dec: 0.00000000
| epoch  44 |   200/  453 batches | lr 0.000000 | ms/batch 245.89 | loss  2.78 | ppl    16.16 | acc     0.45 | train_ae_norm     1.00
[44/100][399/453] Loss_D: 0.17508277 (Loss_D_real: 0.08836772 Loss_D_fake: 0.08671504) Loss_G: 0.08645608 Loss_Enh_Dec: 0.00000000
| epoch  44 |   400/  453 batches | lr 0.000000 | ms/batch 243.89 | loss  2.79 | ppl    16.27 | acc     0.45 | train_ae_norm     1.00
| end of epoch  44 | time: 115.92s | test loss 470.69 | test ppl 2620208309835979103904197029638311041642208915288429102317297752838336221201731741942297597505428446995293989135562377931557789728381576507069275707823904070382599250662866522385890296054258705998500855808.00 | acc 0.432
bleu_self: [1.00000000,1.00000000,1.00000000,1.00000000,1.00000000]
bleu_test: [1.00000000,1.00000000,0.99022791,0.96661062,0.90288645]
| epoch  45 |     0/  453 batches | lr 0.000000 | ms/batch  0.93 | loss  0.01 | ppl     1.01 | acc     0.45 | train_ae_norm     1.00
[45/100][199/453] Loss_D: 0.13071340 (Loss_D_real: 0.09630011 Loss_D_fake: 0.03441329) Loss_G: -0.20193486 Loss_Enh_Dec: 0.00000000
| epoch  45 |   200/  453 batches | lr 0.000000 | ms/batch 245.61 | loss  2.77 | ppl    16.01 | acc     0.45 | train_ae_norm     1.00
[45/100][399/453] Loss_D: 0.08084706 (Loss_D_real: 0.05194180 Loss_D_fake: 0.02890526) Loss_G: -0.14734779 Loss_Enh_Dec: 0.00000000
| epoch  45 |   400/  453 batches | lr 0.000000 | ms/batch 242.85 | loss  2.78 | ppl    16.15 | acc     0.45 | train_ae_norm     1.00
| end of epoch  45 | time: 115.74s | test loss 470.72 | test ppl 2702805206788865984563989669007851396233238986155533493338681084005271068470375369253590543177724600481991981688991492774958526036957804498524771336725826614662181459344163737655730663157240589825093926912.00 | acc 0.432
bleu_self: [1.00000000,1.00000000,1.00000000,1.00000000,1.00000000]
bleu_test: [1.00000000,1.00000000,0.99218233,0.97328850,0.91531240]
New saving model: epoch 045.
| epoch  46 |     0/  453 batches | lr 0.000000 | ms/batch  0.91 | loss  0.01 | ppl     1.01 | acc     0.45 | train_ae_norm     1.00
[46/100][199/453] Loss_D: 0.05516236 (Loss_D_real: 0.04226924 Loss_D_fake: 0.01289312) Loss_G: -0.27594236 Loss_Enh_Dec: 0.00000000
| epoch  46 |   200/  453 batches | lr 0.000000 | ms/batch 243.66 | loss  2.77 | ppl    15.90 | acc     0.45 | train_ae_norm     1.00
[46/100][399/453] Loss_D: 0.05440939 (Loss_D_real: 0.03461362 Loss_D_fake: 0.01979578) Loss_G: -0.32123667 Loss_Enh_Dec: 0.00000000
| epoch  46 |   400/  453 batches | lr 0.000000 | ms/batch 246.24 | loss  2.77 | ppl    16.02 | acc     0.45 | train_ae_norm     1.00
